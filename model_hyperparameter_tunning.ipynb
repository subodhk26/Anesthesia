{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#Deep Learning libraries\n",
    "import keras\n",
    "from keras.layers import Dense, Activation,Dropout, BatchNormalization,LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import sigmoid\n",
    "from keras.losses import binary_crossentropy\n",
    "import talos as ta\n",
    "\n",
    "\n",
    "#Machine Learning libraries\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to tune each model\n",
    "\n",
    "For each model, a function will be defined to perform hyperparameter tunning. Parameter grid will be within the function even though it could also be an input if we wanted to modify it for each variable. Inputs will be the data and the name of the variable used for model output. For Machine Learning models `GridSearchCV` will be used, which check every parameter combination on the list using cross-validation. For Deep Learning models `Talos` will be used, which does the same thing but without cross-validation. The function saved the results including variable name so it can be distinguished later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "For Machine Learning models we just need to initialise the model and then use `GridSearchCV` from `sklearn` with the desired grid. `X` and `y` data are not separated by test and train as cross-validation will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_rf(x,y,variable):\n",
    "\n",
    "\n",
    "    # Create the parameter grid\n",
    "    rf_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [10,30],\n",
    "        'max_features': ['auto'],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "        'min_samples_split': [5,10],\n",
    "        'n_estimators': list(range(100,2001,100))\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create a base model\n",
    "    rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "\n",
    "    # Instantiate the grid search model (cross-validation=4)\n",
    "    rf_search = GridSearchCV(rf, rf_grid, cv = 4, n_jobs = -1, verbose = 2)\n",
    "\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    rf_search.fit(x,y)\n",
    "    \n",
    "    \n",
    "    # Save results\n",
    "    rf_results=pd.DataFrame(rf_search.cv_results_)\n",
    "    rf_results.drop(['params'],axis=1,inplace=True)\n",
    "    rf_results.to_csv('hyperparameter\\\\' + variable +'\\\\RF_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_knn(x,y,variable):\n",
    "\n",
    "\n",
    "    # Create the parameter grid\n",
    "    knn_grid = {'n_neighbors': list(range(100,1001,20)),\n",
    "           'weights': ['uniform', 'distance'],\n",
    "           'leaf_size':[5,30],\n",
    "           'algorithm':['auto', 'ball_tree']}\n",
    "\n",
    "\n",
    "    # Create a base model\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "    # Instantiate the grid search model (cross-validation=4)\n",
    "    knn_search = GridSearchCV(knn, knn_grid,  cv = 4, verbose=2, n_jobs = -1)\n",
    "\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    knn_search.fit(x,y)\n",
    "    \n",
    "    \n",
    "    # Save results\n",
    "    knn_results=pd.DataFrame(knn_search.cv_results_)\n",
    "    knn_results.drop(['params'],axis=1,inplace=True)\n",
    "    knn_results.to_csv('hyperparameter\\\\' + variable + '\\\\kNN_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_svc(x,y,variable):\n",
    "\n",
    "\n",
    "    # Create the parameter grid\n",
    "    svc_grid = {'C': [0.1,1, 10, 100, 1000], \n",
    "                  'gamma': ['auto','scale'],\n",
    "                  'kernel': ['linear','rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "\n",
    "    # Create a base model\n",
    "    svc= SVC()\n",
    "\n",
    "\n",
    "    # Instantiate the grid search model (cross-validation=4)\n",
    "    svc_search = GridSearchCV(svc,svc_grid,cv=4,verbose=2,n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    svc_search.fit(x,y)\n",
    "    \n",
    "    \n",
    "    # Save results\n",
    "    svc_results=pd.DataFrame(svc_search.cv_results_)\n",
    "    svc_results.drop(['params'],axis=1,inplace=True)\n",
    "    svc_results.to_csv('hyperparameter\\\\' + variable + '\\\\SVC_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "For Deep Learning models `Talos` works quite different as we have to manually define the model according to the parameters. After the experiment is run we save the model using the `Deploy` method, which saved the results as well as the best model and its weights in `h5` format, which can be later used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model to be optimized\n",
    "def NN_model(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    \n",
    "    #initialise model\n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    #add hidden layers (dense + dropout)\n",
    "    for i in range(params['hidden_layers']):\n",
    "        \n",
    "        \n",
    "        #first layer must have input dimension according to data\n",
    "        if i==0:\n",
    "            model.add(Dense(params['num_neurons'], input_dim=x_train.shape[1],\n",
    "                            activation=sigmoid,\n",
    "                            kernel_initializer='normal'))\n",
    "\n",
    "            model.add(Dropout(params['dropout']))\n",
    "        \n",
    "        \n",
    "        #rest of hidden layers\n",
    "        else:\n",
    "            model.add(Dense(params['num_neurons'],\n",
    "                            activation=sigmoid,\n",
    "                            kernel_initializer='normal'))\n",
    "\n",
    "            model.add(Dropout(params['dropout']))\n",
    "   \n",
    "\n",
    "    #output layer\n",
    "    model.add(Dense(1, activation=sigmoid,\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    \n",
    "    #compile model\n",
    "    model.compile(loss=binary_crossentropy,\n",
    "                  #add a regulizer normalization function from Talos\n",
    "                  optimizer=Adam(lr=ta.utils.lr_normalizer(params['lr'],Adam)),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    #fit model\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    #output history and model\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_nn(train_X,train_y,test_X,test_y,variable):\n",
    "\n",
    "\n",
    "    #create parameter grid\n",
    "    nn_grid = {'lr': [3.5,5],\n",
    "         'num_neurons':[50,60,70,80,90,100],\n",
    "         'hidden_layers':[2,3],\n",
    "         'batch_size': [16,32,64],\n",
    "         'epochs': [10,50],\n",
    "         'dropout': [0.1,0.2]}\n",
    "    \n",
    "    \n",
    "    #create scan object and search (no cross validation)\n",
    "    nn_search = ta.Scan(x=train_X,\n",
    "                y=train_y,\n",
    "                model=NN_model,\n",
    "                x_val=test_X,\n",
    "                y_val=test_y,\n",
    "                params=nn_grid,\n",
    "                experiment_name='hyper_parameter_NN_' + variable)\n",
    "    \n",
    "    \n",
    "    #save results\n",
    "    ta.Deploy(scan_object=nn_search, model_name= variable + '_NN_results', metric='val_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "For LSTM data must be reshaped as for a 45s input sequence there will be only one output. The function `to_LSTM` does that by grouping X data so that each register has 45points (1 patient), and then reshaping it so that it is (number of patients x 45s x 77 variables) while simplifying output to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to reshape data to match LSTM requirements\n",
    "def to_LSTM(x_data, y_data):\n",
    "    \n",
    "    \n",
    "    #initialise X and y\n",
    "    out=[]\n",
    "    inp=[]\n",
    "    \n",
    "    \n",
    "    #iterate over patients (45s/patient)\n",
    "    for i in range(int(x_data.shape[0]/45)):\n",
    "        \n",
    "        \n",
    "        #reshape X (per patient)\n",
    "        X=(pd.DataFrame(x_data).iloc[45*i:45*(i+1),]).values\n",
    "        inp.append(X.reshape((X.shape[0], 1,X.shape[1])))\n",
    "        \n",
    "        \n",
    "        #turn y to 1 point per patient\n",
    "        if (pd.DataFrame(y_data).iloc[i*45:(i+1)*45]==1).any()[0]:\n",
    "            out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "    \n",
    "    \n",
    "    #reshape X (overall)\n",
    "    inp=np.array(inp)\n",
    "    inp=inp.reshape((inp.shape[0],inp.shape[1],inp.shape[3]))\n",
    "    \n",
    "    \n",
    "    #output X and y\n",
    "    return inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model to be optimized\n",
    "def LSTM_model(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    \n",
    "    #initialise model\n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    #add lstm layer if there must be only one (no return_sequences)\n",
    "    if params['LSTM_layers']==1:\n",
    "        model.add(LSTM(params['num_neurons'], input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "                           activation=sigmoid,kernel_initializer='normal',dropout=params['dropout']))\n",
    "        \n",
    "        \n",
    "    #if there are many layers (return_sequences needed)\n",
    "    else:\n",
    "        \n",
    "        for i in range(params['LSTM_layers']):\n",
    "            \n",
    "            \n",
    "            #if it is the first layer add input shape\n",
    "            if i==0:\n",
    "                model.add(LSTM(params['num_neurons'], input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "                           activation=sigmoid,kernel_initializer='normal',dropout=params['dropout'],\n",
    "                              return_sequences=True))\n",
    "                \n",
    "            \n",
    "            #if it is the last layer return_sequences is not needed\n",
    "            elif (i+1)==params['LSTM_layers']:\n",
    "                model.add(LSTM(params['num_neurons'],activation=sigmoid,kernel_initializer='normal',\n",
    "                          dropout=params['dropout']))\n",
    "                \n",
    "                \n",
    "            #any other layer\n",
    "            else:\n",
    "                model.add(LSTM(params['num_neurons'],activation=sigmoid,kernel_initializer='normal',\n",
    "                          dropout=params['dropout'],return_sequences=True))\n",
    "            \n",
    "            \n",
    "    #add output layer\n",
    "    model.add(Dense(1, activation=sigmoid,\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    \n",
    "    #compile model\n",
    "    model.compile(loss=binary_crossentropy,\n",
    "                  #add a regulizer normalization function from Talos\n",
    "                  optimizer=Adam(lr=ta.utils.lr_normalizer(params['lr'],Adam)),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    #fit model and save history\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    \n",
    "    #output history and model\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lstm(train_X_LSTM,train_y_LSTM,test_X_LSTM,test_y_LSTM,variable):\n",
    "\n",
    "\n",
    "    #create parameter grid\n",
    "    lstm_grid = {'lr': [3.5,5],\n",
    "         'num_neurons':[25,35,45,55,65,75],\n",
    "         'LSTM_layers':[1,2],\n",
    "         'batch_size': [16,32,64],\n",
    "         'epochs': [200,300],\n",
    "         'dropout': [0.1,0.2]}\n",
    "    \n",
    "    \n",
    "    #create scan object and search (no cross validation)\n",
    "    lstm_search = ta.Scan(x=train_X_LSTM,\n",
    "                y=train_y_LSTM,\n",
    "                model=LSTM_model,\n",
    "                x_val=test_X_LSTM,\n",
    "                y_val=test_y_LSTM,\n",
    "                params=lstm_grid,\n",
    "                experiment_name='hyper_parameter_LSTM_'+variable)\n",
    "    \n",
    "    \n",
    "    #save results\n",
    "    ta.Deploy(scan_object=lstm_search, model_name= variable + '_LSTM_results', metric='val_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to tune all models\n",
    "This function works as a `Main()`, it uses the data and variable name as inputs and passes it through tunning functions for each model. It basically tunes every model for given data and stores the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_all(train_X,train_y,test_X,test_y,variable):\n",
    "    \n",
    "    \n",
    "    #merge train and test as for ML methods, cross validation will be used\n",
    "    x=np.vstack((train_X,test_X))\n",
    "    y=np.concatenate((train_y,test_y))\n",
    "    \n",
    "    \n",
    "    #ML models\n",
    "    tune_rf(x,y,variable)\n",
    "    tune_knn(x,y,variable)\n",
    "    tune_svc(x,y,variable)\n",
    "    \n",
    "    \n",
    "    #rehsape X and y data for LSTM\n",
    "    train_X_LSTM,train_y_LSTM=to_LSTM(train_X,train_y)\n",
    "    test_X_LSTM,test_y_LSTM=to_LSTM(test_X,test_y)\n",
    "    \n",
    "    \n",
    "    #DL models\n",
    "    tune_nn(train_X,train_y,test_X,test_y,variable)\n",
    "    tune_lstm(train_X_LSTM,train_y_LSTM,test_X_LSTM,test_y_LSTM,variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune all models for each variable\n",
    "We can use `tune_all` to perform hyperparameter tunning for each variable. First we load the data, the use it as input to run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_X_bis=pd.read_csv('data\\Data_model\\\\train_X_BIS.csv',header=None,index_col=False).values\n",
    "train_X_mov=pd.read_csv('data\\Data_model\\\\train_X_MOV.csv',header=None,index_col=False).values\n",
    "train_X_nibp=pd.read_csv('data\\Data_model\\\\train_X_NIBP.csv',header=None,index_col=False).values\n",
    "\n",
    "train_y_bis=pd.read_csv('data\\Data_model\\\\train_y_BIS.csv',header=None,index_col=False)[0].tolist()\n",
    "train_y_mov=pd.read_csv('data\\Data_model\\\\train_y_MOV.csv',header=None,index_col=False)[0].tolist()\n",
    "train_y_nibp=pd.read_csv('data\\Data_model\\\\train_y_NIBP.csv',header=None,index_col=False)[0].tolist()\n",
    "\n",
    "\n",
    "#test data\n",
    "test_X_bis=pd.read_csv('data\\Data_model\\\\test_X_BIS.csv',header=None,index_col=False).values\n",
    "test_X_mov=pd.read_csv('data\\Data_model\\\\test_X_MOV.csv',header=None,index_col=False).values\n",
    "test_X_nibp=pd.read_csv('data\\Data_model\\\\test_X_NIBP.csv',header=None,index_col=False).values\n",
    "\n",
    "test_y_bis=pd.read_csv('data\\Data_model\\\\test_y_BIS.csv',header=None,index_col=False)[0].tolist()\n",
    "test_y_mov=pd.read_csv('data\\Data_model\\\\test_y_MOV.csv',header=None,index_col=False)[0].tolist()\n",
    "test_y_nibp=pd.read_csv('data\\Data_model\\\\test_y_NIBP.csv',header=None,index_col=False)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "#BIS\n",
    "tune_all(train_X_bis,train_y_bis,test_X_bis,test_y_bis,'BIS')\n",
    "\n",
    "\n",
    "#MOV\n",
    "tune_all(train_X_mov,train_y_mov,test_X_mov,test_y_mov,'MOV')\n",
    "\n",
    "\n",
    "#NIBP\n",
    "tune_all(train_X_nibp,train_y_nibp,test_X_nibp,test_y_nibp,'NIBP')\n",
    "\n",
    "print(\"--- %s minutes ---\" % round((time.time() - start_time)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
